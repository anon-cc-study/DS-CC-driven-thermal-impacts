{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9b400d-69e0-4a73-8f71-08692aa04104",
   "metadata": {},
   "source": [
    "# Create weather-dependent smartds files (at peak demand timestep)\n",
    "\n",
    "This notebook does the following:\n",
    "1. Define path to opendss ditribution data region folder\n",
    "2. Load measured and predicted electricity buildings demand data\n",
    "3. Convert building kw timeseries to smart-ds format kw and kvar loadshapes and max values\n",
    "4. Extract temp at each timestep (hourly)\n",
    "5. Create smart-ds LineCodes.dss and Transformers.dss using temp at each timestep\n",
    "6. Create smart-ds csv profiles, Loads.dss, LoadShapes.dss files using buildings demand predictions\n",
    "7. Create master.dss file with paths to new load.dss and loadshapes.dss + LineCodes.dss and Transformers.dss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54a69a-cedc-4c11-8bd2-a7396e8a58e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9586d1d-6dfb-46da-9759-d60d49ff20e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310c25ba-d6a3-4b28-bf11-6b60fb647853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np \n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from opendssdirect import dss\n",
    "from src import input_ops\n",
    "from src import opendss_ops\n",
    "from src import file_ops\n",
    "from src import df_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330483a-7461-4689-aae1-d2a96de9b171",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load config file with scenarios and parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e30681-17b2-41aa-bc44-ff6b35fcb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name = 'opendss_config1'; config_path = f\"/home/aviadf/opendss/notebooks/config/{config_file_name}.yaml\"; config = input_ops.load_config(config_path)\n",
    "\n",
    "TGW_years_scenarios = config['TGW_years_scenarios'] \n",
    "\n",
    "CITY_REGIONS_TO_RUN = config['CITY_REGIONS_TO_RUN']\n",
    "    \n",
    "demand_mode = config['demand_mode'] \n",
    "line_rating_mode = config['line_rating_mode'] p\n",
    "transformers_rating_mode = config['transformers_rating_mode'] \n",
    "\n",
    "aggregation_level = config['aggregation_level']\n",
    "\n",
    "smart_ds_year = config['smart_ds_years'][0]\n",
    "start_month = config['start_month']\n",
    "end_month = config['end_month']\n",
    "smart_ds_load_path = config['smart_ds_load_path'] + f\"/{smart_ds_year}\" # path to procesed smart-ds resstock data \n",
    "\n",
    "input_data_prediction_path = config['input_data_prediction_path']\n",
    "\n",
    "solution_mode = config['solution_mode'] # yearly | snapshot\n",
    "\n",
    "## Define variables to create list of mdh to run\n",
    "start_month_mdh = config['start_month_mdh'] \n",
    "end_month_mdh = config['end_month_mdh']\n",
    "top_percent_mdh = config['top_percent_mdh']\n",
    "\n",
    "## Load dictionary, sort by total city aggregated buildings demand, extract mdh of top % load hours\n",
    "## load demand data \n",
    "regional_demand_weather_ampacity_path = smart_ds_load_path + f\"/all_cities/aggregated_demand\"\n",
    "regional_demand_weather_ampacity_all_cities = joblib.load(os.path.join(regional_demand_weather_ampacity_path, \"regional_demand_weather_ampacity_all_cities\"))\n",
    "## Sort your dictionary by aggregated total demand  \n",
    "regional_demand_weather_ampacity_all_cities_sorted = df_ops.sort_nested_dict_dfs(regional_demand_weather_ampacity_all_cities, \"aggregated_predicted_buildings_total_kw\", ascending=False)\n",
    "top_n_hours = int(np.ceil(8760*top_percent_mdh/100)) # calculate top city demand hours to run (top_percent_mdh% of hours of the year)\n",
    "## Define start and end load hours to run\n",
    "start_row_percent = config['start_row_percent'] # define percentage of last run to avoid running same mdh again, e.g., start_row_percent = 1 means that loop will start from the 99th percentile hours\n",
    "start_row_idx = int(np.ceil(8760*start_row_percent/100)) # index from which to start loop of load hours\n",
    "# start_row_idx = 0 # index from which to start loop of load hours\n",
    "end_row_idx = top_n_hours \n",
    "\n",
    "print(f\"TGW_years_scenarios: {TGW_years_scenarios} \\nsmart_ds_year:{smart_ds_year} \\nsolution_mode:{solution_mode} \\ncity region: {CITY_REGIONS_TO_RUN} \\n Demand mode: {demand_mode} \\nline_rating_mode: {line_rating_mode} \\ntransformers_rating_mode: {transformers_rating_mode}\")\n",
    "\n",
    "print(f\"\\n\\ntop_percent_mdh: {top_percent_mdh}%, top_n_hours: {top_n_hours}, start_row_idx:{start_row_idx} ({start_row_percent}%), end_row_idx:{end_row_idx} ({top_percent_mdh}%) \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69208684-f031-41e4-97a6-611ff016d470",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create and compare mdh for different climate scenario and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bf1e47-babe-407a-a7a4-d1c2c1a62964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_mdh = {}\n",
    "for TGW_weather_year, TGW_scenarios in TGW_years_scenarios.items():\n",
    "    for TGW_scenario in TGW_scenarios:\n",
    "        list_of_mdh[(TGW_weather_year, TGW_scenario)] = {}\n",
    "        print(f\"--- Creating list for TGW_scenario:{TGW_scenario} TGW_weather_year:{TGW_weather_year} --- \\n\")\n",
    "        for city, regions in CITY_REGIONS_TO_RUN.items():\n",
    "            ## create list of mdh for top % hours\n",
    "            df_city = regional_demand_weather_ampacity_all_cities_sorted[(TGW_weather_year, TGW_scenario)][city]   # load regional demand data to later create list of mdh for top % hours  \n",
    "            list_of_mdh[(TGW_weather_year, TGW_scenario)][(city)] = df_ops.get_top_n_mdh(df_city, top_n_hours, start_month_mdh, end_month_mdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87171c38-9723-4f03-8244-bebbb60c4576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: pick two lists of mdh\n",
    "mdh_list_1 = list_of_mdh[('2058', 'rcp45hotter')]['GSO']\n",
    "mdh_list_2 = list_of_mdh[('2058', 'rcp85hotter')]['GSO']\n",
    "\n",
    "# Convert to sets for comparison\n",
    "set1 = set(mdh_list_1)\n",
    "set2 = set(mdh_list_2)\n",
    "\n",
    "# Find intersections and differences\n",
    "common = set1 & set2\n",
    "only_in_1 = set1 - set2\n",
    "only_in_2 = set2 - set1\n",
    "\n",
    "# Show results side by side in a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df_compare = pd.DataFrame({\n",
    "    \"Only in (2058,rcp45hotter)\": sorted(list(only_in_1)),\n",
    "    \"Only in (2058,rcp85hotter)\": sorted(list(only_in_2)),\n",
    "})\n",
    "\n",
    "print(\"Common MDH (present in both):\")\n",
    "print(sorted(list(common)))\n",
    "print(\"\\nDifferences side by side:\")\n",
    "print(df_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61adad2c-8ace-43ed-81dc-7f20679c577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_mdh_calendar_grid(list_of_mdh, city, key1, key2, dummy_year=2000):\n",
    "    mdh_list_1 = list_of_mdh[key1][city]\n",
    "    mdh_list_2 = list_of_mdh[key2][city]\n",
    "\n",
    "    set1, set2 = set(mdh_list_1), set(mdh_list_2)\n",
    "    both  = sorted(set1 & set2, key=lambda t: (t[0], t[1], t[2]))\n",
    "    only1 = sorted(set1 - set2, key=lambda t: (t[0], t[1], t[2]))\n",
    "    only2 = sorted(set2 - set1, key=lambda t: (t[0], t[1], t[2]))\n",
    "\n",
    "    def to_dt(triple):\n",
    "        m, d, h = triple\n",
    "        return pd.Timestamp(dummy_year, m, d, h)\n",
    "\n",
    "    # Prepare x (datetime) and y (hour)\n",
    "    x1 = [to_dt(t) for t in only1]\n",
    "    y1 = [ts.hour for ts in x1]\n",
    "\n",
    "    xb = [to_dt(t) for t in both]\n",
    "    yb = [ts.hour for ts in xb]\n",
    "\n",
    "    x2 = [to_dt(t) for t in only2]\n",
    "    y2 = [ts.hour for ts in x2]\n",
    "\n",
    "    plt.figure(figsize=(11, 5))\n",
    "    plt.scatter(x1, y1, label=f\"Only {key1}\", s=14)\n",
    "    plt.scatter(xb, yb, label=\"Both\",        s=14)\n",
    "    plt.scatter(x2, y2, label=f\"Only {key2}\", s=14)\n",
    "\n",
    "    plt.gca().set_ylim(-0.5, 23.5)\n",
    "    plt.gca().invert_yaxis()  # midnight at bottom (optional)\n",
    "    plt.yticks(range(0, 24, 2))\n",
    "\n",
    "    # Format x-axis as month-day, in correct chronological order\n",
    "    plt.gca().set_xticks(sorted(set(x1 + xb + x2)))\n",
    "    plt.gca().set_xticklabels(\n",
    "        [dt.strftime(\"%m-%d\") for dt in sorted(set(x1 + xb + x2))],\n",
    "        rotation=45, ha=\"right\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Date (MM-DD)\")\n",
    "    plt.ylabel(\"Hour of day\")\n",
    "    plt.title(f\"MDH Calendar Grid — {city}: {key1} vs {key2}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example call:\n",
    "plot_mdh_calendar_grid(list_of_mdh, 'GSO', ('2018','historical'), ('2058','rcp85hotter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43296570-e018-4f73-a4d6-98e581a4d592",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dd12e0-a802-47c2-a7db-33e36734407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define parameters for resistance model ---\n",
    "T0 = 20 # default temperature for resistance values in SMART-DS\n",
    "alpha_r = 0.00403 # temperature coefficient default for power line \n",
    "f_amp_temp = 0.012 # ampacity change per degree celsuis (%/C)\n",
    "near_worst_stat = 'p99' # options: avg_daily_max_august | p98 | p99 | p995 | p999 (assumption for ampacity near-worst weather condition baseline. p99 is typically used by utilities and is the CIGRE guideline)\n",
    "regional_peak_start_month = 6; regional_peak_end_month = 9 # inputs to get_regional_peak_times() function, which finds the regional peak timestep during these months\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"--- Solution mode: {solution_mode} Demand mode:{demand_mode} Line rating mode:{line_rating_mode} --- \\n\")\n",
    "\n",
    "# Exit program if solution_mode is different than snapshot mode\n",
    "if solution_mode != \"snapshot\":\n",
    "    sys.stderr.write(f\"ERROR: solution_mode must be 'snapshot', got {solution_mode!r}.\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "for TGW_weather_year, TGW_scenarios in TGW_years_scenarios.items():\n",
    "    for TGW_scenario in TGW_scenarios:\n",
    "        print(f\"--- TGW_scenario:{TGW_scenario} TGW_weather_year:{TGW_weather_year} --- \\n\")\n",
    "        for city, regions in CITY_REGIONS_TO_RUN.items():\n",
    "            \n",
    "            # Load TGW weather data for TGW location (city)\n",
    "            TGW_location = {\"GSO\": \"Greensboro\",\"AUS\": \"Austin\",\"SFO\": \"SanFrancisco\"}.get(city, city)\n",
    "            TGW_weather_df_save_path = f\"{input_data_prediction_path}/{TGW_location}/{TGW_scenario}/\"\n",
    "            TGW_weather_df = joblib.load(os.path.join(TGW_weather_df_save_path,f\"TGW_weather_{TGW_weather_year}.joblib\"))\n",
    "            \n",
    "            # load near-worst historical temperature for TGW location (city) to set default ambient temp of power lines ampacity\n",
    "            TGW_stats_dir = f\"main_folder/TGW/TGW_Distribution_for_Aviad/TGW_weather_statistics\"\n",
    "            loaded_temp_stats = joblib.load(os.path.join(TGW_stats_dir, f\"temperature_stats.joblib\"))\n",
    "            Ta_near_worst = loaded_temp_stats[TGW_location].loc[(loaded_temp_stats[TGW_location]['scenario'] == 'historical') & (loaded_temp_stats[TGW_location]['year'] == int(smart_ds_year)), near_worst_stat].values[0]\n",
    "            print(f\"Near-worst ({near_worst_stat}) historical local temperature found for TGW_scenario historical TGW_weather_year {smart_ds_year} {city} is {Ta_near_worst} (used as base temp for line derating)\\n\")\n",
    "\n",
    "            ## create list of mdh for top % hours\n",
    "            df_city = regional_demand_weather_ampacity_all_cities_sorted[(TGW_weather_year, TGW_scenario)][city]   # load regional demand data to later create list of mdh for top % hours  \n",
    "            list_of_mdh = df_ops.get_top_n_mdh(df_city, top_n_hours, start_month_mdh, end_month_mdh)\n",
    "\n",
    "            for region in regions:\n",
    "                print(f\"--- city: {city}, region: {region} ---\\n\")\n",
    "\n",
    "                region_start_time = time.time()\n",
    "                \n",
    "                # Define path to opendss region folder (assuming snapshot mode)\n",
    "                region_path = f\"main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/scenarios/base_timeseries/opendss_no_loadshapes\"\n",
    "\n",
    "                # --- Load demand data (regional data at the buildings level) ---\n",
    "                ### Load measured buildings' demand timeseries dataframe, organized by feeders (single region) | colums: total_site_electricity_kw\tpf\tcooling_sum_kw\theating_kw\tnon_cool_n_heat_kw\n",
    "                input_data_region_dir = f'{smart_ds_load_path}/{city}/{region}/buildings'\n",
    "                measured_buildings_cool_heat_dict = joblib.load(os.path.join(input_data_region_dir, \"measured_buildings_cool_heat_dict.joblib\")) # Save the file\n",
    "                ### Load predicted buildings' total demand panda timeseries, organized by feeders (single region) \n",
    "                predictions_path = f\"main_folder/load_prediction/results/data/prediction/output/{config['smart_ds_years'][0]}/months_{config['start_month']}_{config['end_month']}/cooling_n_heating/{config['X_columns_set']}/{config['aggregation_level']}/\" \n",
    "                predictions_dir = os.path.join(predictions_path, f\"{TGW_scenario}/predictions/{city}/{region}/\")\n",
    "                building_predicted_total_dict = joblib.load(os.path.join(predictions_dir, f\"{demand_mode}_TGW_{TGW_weather_year}_buildings_dict.joblib\"))\n",
    "\n",
    "                print(f\"Loaded measured and predicted buildings' demand timeseries\\n\")\n",
    "\n",
    "                # --- Pre-Process data (Convert ResStock building kw timeseries to smart-ds kw and kvar loadshapes and max values) ---\n",
    "\n",
    "                ### Create predicted kvar from predicted kw and measured pf - create predicted reactive load profiles (kvar) using measured power factor time series and predicted kw | each resulting df has columns: date_time kw kvar\n",
    "                for outer_key, inner_dict in building_predicted_total_dict.items():   \n",
    "                    for building_name, kw_series in inner_dict.items():\n",
    "                        # Convert active power series to DataFrame\n",
    "                        df = kw_series.to_frame(name='kw')\n",
    "\n",
    "                        # Get the matching power factor series\n",
    "                        pf_series = measured_buildings_cool_heat_dict[outer_key][building_name]['pf']\n",
    "\n",
    "                        # Align indices if needed\n",
    "                        pf_series = pf_series.loc[df.index]\n",
    "\n",
    "                        # Calculate reactive power\n",
    "                        angle_rad = np.arccos(pf_series.clip(lower=0.01, upper=1.0))  # prevent domain error\n",
    "                        df['kvar'] = df['kw'] * np.tan(angle_rad)\n",
    "\n",
    "                        # Store the result\n",
    "                        building_predicted_total_dict[outer_key][building_name] = df\n",
    "\n",
    "                print(\"Created predicted kvar from predicted kw and measured pf \\n\")\n",
    "\n",
    "                ### Create measured kw and kvar max from measured kw and pf - Use measured kw and measured power factor time series to create measured kw and kvar max\n",
    "                # Initialize dictionary to store max values\n",
    "                building_measured_max_dict = {} \n",
    "                for outer_key, inner_dict in measured_buildings_cool_heat_dict.items():\n",
    "                    building_measured_max_dict[outer_key] = {}\n",
    "\n",
    "                    for building_name, df in inner_dict.items():\n",
    "\n",
    "                        # compute kvar timeseries\n",
    "                        # Get the matching power factor series\n",
    "                        pf_series = measured_buildings_cool_heat_dict[outer_key][building_name]['pf']\n",
    "                        pf_series = pf_series.loc[df.index] # Align indices if needed\n",
    "\n",
    "                        # Calculate reactive power\n",
    "                        angle_rad = np.arccos(pf_series.clip(lower=0.01, upper=1.0))  # prevent domain error\n",
    "                        df['total_site_electricity_kvar'] = df['total_site_electricity_kw'] * np.tan(angle_rad)\n",
    "\n",
    "                        # Compute max values\n",
    "                        kw_max = df['total_site_electricity_kw'].max()\n",
    "                        kvar_max = df['total_site_electricity_kvar'].max()\n",
    "\n",
    "                        # Store max values\n",
    "                        building_measured_max_dict[outer_key][building_name] = {\n",
    "                            'kw_max': kw_max,\n",
    "                            'kvar_max': kvar_max\n",
    "                        }\n",
    "\n",
    "                print(\"Created measured kw and kvar max from measured kw and pf \\n\")\n",
    "                \n",
    "                # Free unused memory\n",
    "                del measured_buildings_cool_heat_dict\n",
    "                gc.collect()\n",
    "                \n",
    "                ## Get lists of months, days and hours from a sample dataframe in building_predicted_total_dict (used later to mask selected mdh)\n",
    "                first_outer_key, first_inner_dict = next(iter(building_predicted_total_dict.items())) # get the first outer_key and its inner dict\n",
    "                first_building_name, first_df = next(iter(first_inner_dict.items())) # get the first building_name and df\n",
    "                months = df.index.month; days   = df.index.day; hours  = df.index.hour\n",
    "                \n",
    "                # Initialize dictionary to store max values\n",
    "                building_predicted_max_dict = {}\n",
    "                                \n",
    "                # find regional kw and kvar peak timestep (timestamp) using building_predicted_total_dict\n",
    "                regional_peak_times = df_ops.get_regional_peak_times(building_predicted_total_dict, regional_peak_start_month, regional_peak_end_month)\n",
    "                print(f\"Extracted regional peak time: {regional_peak_times}\\n\")\n",
    "\n",
    "                for outer_key, inner_dict in building_predicted_total_dict.items():\n",
    "                    building_predicted_max_dict[outer_key] = {}\n",
    "\n",
    "                    for building_name, df in inner_dict.items():\n",
    "                        # Compute max values\n",
    "                        kw_max = df['kw'].max()\n",
    "                        kvar_max = df['kvar'].max()\n",
    "\n",
    "                        # Compute kw kvar values at regional peak timestep\n",
    "                        kw_at_region_peak = df['kw'].loc[regional_peak_times['kw_regional_peak_time']]\n",
    "                        kvar_at_region_peak = df['kvar'].loc[regional_peak_times['kvar_regional_peak_time']]\n",
    "\n",
    "                        # Store max values\n",
    "                        building_predicted_max_dict[outer_key][building_name] = {\n",
    "                            'kw_max': kw_max,\n",
    "                            'kvar_max': kvar_max,\n",
    "                            'kw_at_region_peak': kw_at_region_peak,\n",
    "                            'kvar_at_region_peak': kvar_at_region_peak,\n",
    "                        }\n",
    "\n",
    "                print(\"Converted predicted kw and kvar timeseries to load shapes (0-1) values, max values and values at the regional peak\\n\")\n",
    "                \n",
    "\n",
    "                dss.Command(f'Redirect \"{region_path}/Master.dss\"')\n",
    "                print(f\"Redirected dss engine to {region_path}/Master.dss\")\n",
    "                \n",
    "                # Organize all building_type entries under each feeder (for Load.dss process)\n",
    "                feeder_dict = defaultdict(dict)\n",
    "                for key, building_dict in building_predicted_total_dict.items():\n",
    "                    smart_ds_year, city, region, feeder, building_type = key\n",
    "                    feeder_key = (smart_ds_year, city, region, feeder)\n",
    "                    feeder_dict[feeder_key][building_type] = building_dict\n",
    "                                             \n",
    "\n",
    "                # Create a list of paths to original folders with linecodes.dss / Transformers.dss / Loads.dss (that's why we use max depth 3)\n",
    "                folders_with_linecodes = file_ops.find_folders_with_file(region_path, \"LineCodes.dss\", max_depth=3)\n",
    "                folders_with_transformers = file_ops.find_folders_with_file(region_path, \"Transformers.dss\", max_depth=3)\n",
    "                # Get list of original feeder paths (folders with Loads.dss)\n",
    "                feeder_folders = file_ops.find_folders_with_file(region_path, \"Loads.dss\")\n",
    "\n",
    "                # --- Iterate over all selected month-day-hour (mdh) --- \n",
    "                for row_i in range(start_row_idx, end_row_idx):\n",
    "                    mdh = list_of_mdh[row_i]; m,d,h = mdh\n",
    "                    print(f\"Creating files for month:{m} hour:{d} day:{h}\\n\")\n",
    "\n",
    "                    mdh_mask = (months == m) & (days == d) & (hours == h) # mask to select kw value for each building at the selected mdh\n",
    "\n",
    "                    ## Extract temperature selected time \n",
    "                    # Filter TGW_weather_df rows by matching month, day, and hour\n",
    "                    matched = TGW_weather_df[(TGW_weather_df['date_time'].dt.month == m) & (TGW_weather_df['date_time'].dt.day == d) & (TGW_weather_df['date_time'].dt.hour == h)]\n",
    "                    # Extract temperature at m, d, h\n",
    "                    Ta = int(matched[['Dry Bulb Temperature [°C]']].iloc[0, 0])\n",
    "                    print(f\"Found Temperature for TGW_scenario {TGW_scenario} TGW_weather_year {TGW_weather_year} {city} {region}: {Ta} (used for derating and resistance)\\n\")\n",
    "\n",
    "                    # Modify lines' thermal capacity and resistance in LineCodes files [currently uses an approximation for ampacity change]\n",
    "                    # Calculate resistance and ampacity temperature-based factors\n",
    "                    Rmatrix_factor = 1 + (alpha_r * (Ta - T0)) # Define Rmatrix multiplier based on T-R relationship\n",
    "                    amp_factor = (1 - ((Ta - Ta_near_worst) * f_amp_temp) )\n",
    "\n",
    "                    ## ****** modify functions to change name convention to finish with _m_d_h.dss ****** ##\n",
    "                    for folder_path in folders_with_linecodes:\n",
    "                        # opendss_ops.modify_LineCodes(folder_path, Rmatrix_factor, Ta)\n",
    "                        opendss_ops.modify_LineCodes(folder_path, Rmatrix_factor, amp_factor, line_rating_mode, TGW_scenario, TGW_weather_year,m,d,h)\n",
    "                    # Modify transformers' thermal capacity (KVA rating values) in Transformers files  [using IEEEc57.91 Table 3 approximation] \n",
    "                    for folder_path in folders_with_transformers:\n",
    "                        opendss_ops.modify_tranformers(folder_path, Ta, transformers_rating_mode, TGW_scenario, TGW_weather_year,m,d,h)\n",
    "                    print(f\"Saved modified line codes and transformers\")\n",
    "\n",
    "\n",
    "                    ### Create load.dss with load at selected mdh \n",
    "                    phase_load_max_dict = {}  # Initialize phase load dictionary \n",
    "                    # Iterate over each feeder once (rather than once per building type, like other dictionaries in this process)\n",
    "                    for feeder_key, building_types_dict in feeder_dict.items():\n",
    "                        smart_ds_year, city, region, feeder = feeder_key\n",
    "                        phase_load_max_dict[feeder_key] = {}  # Initialize phase load internal dictionary, e.g., feeder_key = ('2018', 'GSO', 'rural', 'rhs2_1247--rdt1262')\n",
    "                        # use feeder_key and feeder_folders to get path to loads.dss\n",
    "                        # Use regex to find the exact feeder folder\n",
    "                        feeder_pattern = re.compile(rf'(^|/){re.escape(feeder)}(/|$)')\n",
    "                        matching_feeders = [f for f in feeder_folders if feeder_pattern.search(f)]\n",
    "                        if not matching_feeders:\n",
    "                            print(f\"[WARNING] Feeder folder not found for feeder: {feeder}\")\n",
    "                            continue\n",
    "                        feeder_folder = matching_feeders[0]\n",
    "                        path_to_loads_dss = os.path.join(feeder_folder,\"Loads.dss\") # path to original Loads.dss, e.g., main_folder/SMART-DS/v1.0/2018/GSO/rural/scenarios/base_timeseries/opendss/rhs2_1247/rhs2_1247--rdt1262/Loads.dss\n",
    "\n",
    "                        # create scenario-based path (Feeder_folder/predicted_loads/TGW/climate_scnario/weather_year/)\n",
    "                        output_dir = os.path.join(feeder_folder, \"predicted_loads\", \"TGW\", TGW_scenario, TGW_weather_year)\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                        # Create copy of loads.dss in scenario-based path\n",
    "                        original_loads_path = path_to_loads_dss # Path to original loads file\n",
    "\n",
    "                        ## ****** change name convention to finish with _m_d_h.dss****** ##\n",
    "                        new_loads_path = output_dir + f\"/Loads_{m}_{d}_{h}.dss\"    # Path to new loads file\n",
    "\n",
    "                        shutil.copyfile(original_loads_path, new_loads_path) # Duplicate loads file\n",
    "\n",
    "                        with open(original_loads_path, \"r\") as infile, open(new_loads_path, \"w\") as outfile:\n",
    "                            for line in infile:     ## Loop over all rows in loads.dss (all phase loads)\n",
    "                                if line.startswith(\"New Load.\"):\n",
    "                                    # --- Extract data from the Loads.dss line ---\n",
    "                                    # Extract resstock building name (in the format of loadShape names, e.g., res_kw_626_pu)\n",
    "                                    loadshape_pattern = re.search(r'yearly=(\\w+)_(kw|kvar)_(\\d+)_pu', line) # res_kw_452_pu returns: loadshape_pattern.group(1) = res | loadshape_pattern.group(2) = kw | loadshape_pattern.group(3) = 452 \n",
    "                                    # Extract load phase name, e.g., load_p1rlv5636_2\n",
    "                                    phase_load_name_pattern = re.search(r'Load\\.(\\S+)\\s', line)\n",
    "                                    # Make sure patterns were found\n",
    "                                    if not loadshape_pattern or not phase_load_name_pattern:\n",
    "                                        raise ValueError(\"Failed to extract required fields from Loads.dss line\")\n",
    "                                    # Use patterns\n",
    "                                    building_loadshape_name = f\"{loadshape_pattern.group(1)}_{loadshape_pattern.group(2)}_{loadshape_pattern.group(3)}_pu_{feeder}\" # e.g., res_kw_452_pu\n",
    "                                    phase_load_name = phase_load_name_pattern.group(1)\n",
    "                                    # Extract measured kw kvar load phase values \n",
    "                                    pattern = r'kW=([+-]?[0-9]*\\.?[0-9]+(?:[eE][+-]?[0-9]+)?)\\s+kvar=([+-]?[0-9]*\\.?[0-9]+(?:[eE][+-]?[0-9]+)?)'  # Regex pattern to find kW and kvar values\n",
    "                                    # Search for pattern\n",
    "                                    match = re.search(pattern, line)\n",
    "                                    if match:\n",
    "                                        measured_load_phase_kw_max = float(match.group(1))\n",
    "                                        measured_load_phase_kvar_max = float(match.group(2))\n",
    "\n",
    "                                    # Initialize inner dictionary using phase load name\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name] = {}\n",
    "\n",
    "                                    # Get building type and name and outer key\n",
    "                                    building_name = f\"{loadshape_pattern.group(1)}_{loadshape_pattern.group(3)}\" # , e.g., res_626\n",
    "                                    building_type = building_name.split('_')[0] # e.g., res\n",
    "                                    outer_key = (feeder_key[0],feeder_key[1],feeder_key[2],feeder_key[3],building_type)\n",
    "\n",
    "                                    ## ******  use building_name to get kw kvar at selected mdh  ****** ##\n",
    "                                    # Add data to phase load dictionary: phase load name | resstock name |  measured load phase kw max | measured load phase kvar max\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['phase_load_name'] = phase_load_name\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['building_name'] = building_name\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['load_shape_name'] = building_loadshape_name\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['measured_kw'] = measured_load_phase_kw_max\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['measured_kvar'] = measured_load_phase_kvar_max\n",
    "\n",
    "                                    # load measured and predicted resstock kw and kvar max\n",
    "                                    measured_ressstock_kw_max = building_measured_max_dict[outer_key][building_name]['kw_max']\n",
    "                                    measured_ressstock_kvar_max = building_measured_max_dict[outer_key][building_name]['kvar_max']\n",
    "\n",
    "\n",
    "                                    # Load predicted resstock kw and kvar max and value at regional peak timestep\n",
    "                                    predicted_ressstock_kw_at_region_peak = building_predicted_max_dict[outer_key][building_name]['kw_at_region_peak'] \n",
    "                                    predicted_ressstock_kvar_at_region_peak = building_predicted_max_dict[outer_key][building_name]['kvar_at_region_peak']\n",
    "                                    predicted_ressstock_kw_max = building_predicted_max_dict[outer_key][building_name]['kw_max'] \n",
    "                                    predicted_ressstock_kvar_max = building_predicted_max_dict[outer_key][building_name]['kvar_max'] \n",
    "\n",
    "\n",
    "                                    # Compute kw kvar values at selected mdh\n",
    "                                    building_df = building_predicted_total_dict[outer_key][building_name]\n",
    "                                    mdh_mask = (months == m) & (days == d) & (hours == h)\n",
    "                                    kw_vals = building_df.loc[mdh_mask, 'kw'] # get all kw with mdh match to mask\n",
    "                                    kvar_vals = building_df.loc[mdh_mask, 'kvar'] # get all kvar  with mdh match to mask\n",
    "                                    predicted_ressstock_kw_at_mdh = kw_vals.iloc[0] # kw value at first mdh match \n",
    "                                    predicted_ressstock_kvar_at_mdh = kvar_vals.iloc[0] # kvar value at first mdh match \n",
    "                                    \n",
    "                                    predicted_ressstock_kw_at_mdh = kw_vals.iloc[0] # kw value at first mdh match \n",
    "                                    predicted_ressstock_kvar_at_mdh = kvar_vals.iloc[0] # kvar value at first mdh match \n",
    "\n",
    "                                    # Calculate predicted load phase kw kvar values \n",
    "                                    if measured_ressstock_kw_max == 0 or measured_ressstock_kvar_max == 0:\n",
    "                                        raise ValueError(f\"[ERROR] Zero measured max for {building_name} in {feeder_key}\")\n",
    "                                    predicted_load_phase_kw_max = predicted_ressstock_kw_max * (measured_load_phase_kw_max / measured_ressstock_kw_max)\n",
    "                                    predicted_load_phase_kvar_max = predicted_ressstock_kvar_max * (measured_load_phase_kvar_max / measured_ressstock_kvar_max)\n",
    "                                    predicted_load_phase_kw_at_region_peak = predicted_ressstock_kw_at_region_peak * (measured_load_phase_kw_max / measured_ressstock_kw_max)\n",
    "                                    predicted_load_phase_kvar_at_region_peak = predicted_ressstock_kvar_at_region_peak * (measured_load_phase_kvar_max / measured_ressstock_kvar_max)\n",
    "                                    predicted_load_phase_kw_at_mdh = predicted_ressstock_kw_at_mdh * (measured_load_phase_kw_max / measured_ressstock_kw_max)\n",
    "                                    predicted_load_phase_kvar_at_mdh = predicted_ressstock_kvar_at_mdh * (measured_load_phase_kvar_max / measured_ressstock_kvar_max)\n",
    "\n",
    "                                    # Add predicted kw kvar values to dictioanry\n",
    "                                    # data in dictionary will be: phase load name | resstock name |  measured load phase kw max | measured load phase kvar max | predicted load phase kw max | predicted load phase kvar max | predicted load phase kw regional peak | predicted load phase kvar regional peak\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['predicted_kw_max'] = predicted_load_phase_kw_max\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['predicted_kvar_max'] = predicted_load_phase_kvar_max\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['predicted_kw_at_region_peak'] = predicted_load_phase_kw_at_region_peak\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['predicted_kvar_at_region_peak'] = predicted_load_phase_kvar_at_region_peak\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['predicted_kw_at_mdh'] = predicted_load_phase_kw_at_mdh\n",
    "                                    phase_load_max_dict[feeder_key][phase_load_name]['predicted_kvar_at_mdh'] = predicted_load_phase_kvar_at_mdh\n",
    "\n",
    "                                    predicted_load_phase_kw = predicted_load_phase_kw_at_mdh\n",
    "                                    predicted_load_phase_kvar = predicted_load_phase_kvar_at_mdh\n",
    "\n",
    "                                    # Modify loads.dss line with predicted kw kvar values and new resstock load shape names 'res_#_feeder' (add feeder name to existing yearly='')\n",
    "                                    # Update kW and kvar values in the line\n",
    "                                    line = re.sub(r\"kW=([0-9\\.]+)\", lambda m: f\"kW={predicted_load_phase_kw}\", line)\n",
    "                                    line = re.sub(r\"kvar=([0-9\\.]+)\", lambda m: f\"kvar={predicted_load_phase_kvar}\", line)\n",
    "                                    line = re.sub(r'(yearly=[^\\s\\n]+)', r'\\1_' + feeder, line)\n",
    "\n",
    "                                # Modify loads.dss in scenario-based path with the new line\n",
    "                                outfile.write(line)\n",
    "                    print(f\"Saved modified Load.dss files\\n\")\n",
    "\n",
    "                    ## Create a duplicate master file with paths to new load.dss and loadshapes.dss\n",
    "                    original_master_file = region_path + \"/Master.dss\" # Path to original master file\n",
    "                    new_master_dir = os.path.join(region_path, \"predicted_master_files\", TGW_scenario, TGW_weather_year)\n",
    "                    os.makedirs(new_master_dir, exist_ok=True)                 # Create directory if it does not exist\n",
    "                    new_master_file = new_master_dir + f\"/Master_{TGW_scenario}_{TGW_weather_year}_{m}_{d}_{h}.dss\"    # Path to new master file\n",
    "                    shutil.copyfile(original_master_file, new_master_file) # Duplicate master file\n",
    "                    opendss_ops.modify_master_file(new_master_file, solution_mode, demand_mode, line_rating_mode, transformers_rating_mode, TGW_scenario, TGW_weather_year,m,d,h)\n",
    "\n",
    "                    print(f\"A modified master file was created in {new_master_file}\\n\")\n",
    "  \n",
    "                # Free unused memory\n",
    "                del phase_load_max_dict \n",
    "                gc.collect() \n",
    "                \n",
    "                # Free unused memory after every region run\n",
    "                del building_predicted_total_dict\n",
    "                del building_predicted_max_dict\n",
    "                gc.collect()\n",
    "                \n",
    "                region_end_time = time.time(); print(f\"---Runtime for {city} {region}: {(region_end_time - region_start_time) / 60:.2f} minutes---\\n\")\n",
    "                \n",
    "end_time = time.time(); print(f\"--- Total Runtime: {(end_time - start_time) / 60:.2f} minutes ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendss_env2",
   "language": "python",
   "name": "opendss_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
