{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910cb6d8-24f9-4418-986e-8905cfe9d239",
   "metadata": {},
   "source": [
    "# Create Load data for disaggregration process of feeder level cooling and heating predictions\n",
    "\n",
    "This notebook does the following:\n",
    "1. Import load data (from ResStock Parquet files in SMART-DS)\n",
    "2. Filter parquet_files based on feeder's loads.dss \n",
    "3. Create columns: total demand, cooling sum, heating and non cooling-heating load\n",
    "4. Save as dictionary, clustered by feeders (for disaggregration of feeder level cooling and heating predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f86aa3-4f93-48a7-95d1-e6c82a0a2ca3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f278ee54-e161-4b4b-ad5a-f74d4ee829f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from pandas import DatetimeIndex\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import pytz\n",
    "\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "from src import input_ops\n",
    "from src import model_ops\n",
    "from src import aux_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416dea5-9704-4872-8ba2-67a77cb139fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load config file with scenarios and parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6e186f-1eb4-49ac-83e7-50dd181e1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name = 'config1'; config_path = f\"config/{config_file_name}.yaml\"; config = input_ops.load_config(config_path)\n",
    "\n",
    "# lists of parameters to use for TGW df creation\n",
    "TGW_scenario = config['TGW_scenario']\n",
    "TGW_weather_years = config['TGW_weather_years']\n",
    "prediction_model_str = config['prediction_model']\n",
    "aggregation_level = config['aggregation_level']\n",
    "\n",
    "X_columns = config[config['X_columns_set']]\n",
    "CITY_REGIONS_TO_RUN = config['CITY_REGIONS_TO_RUN']\n",
    "\n",
    "input_data_dict_name = config['input_data_dict_name']\n",
    "aggregation_level = config['aggregation_level']\n",
    "smart_ds_year = config['smart_ds_years'][0]\n",
    "building_types = config[\"building_types\"]\n",
    "\n",
    "input_data_training_path = config['input_data_training_path']\n",
    "CITY_REGIONS_TO_RUN = config['CITY_REGIONS_TO_RUN']\n",
    "start_month = config['start_month']\n",
    "end_month = config['end_month']\n",
    "\n",
    "## Initialize parameters for saving paths\n",
    "Y_column = config['Y_column']\n",
    "input_data_prediction_path = config['input_data_prediction_path']\n",
    "output_path_prediction_str = config['output_data_prediction_path']\n",
    "\n",
    "smart_ds_year = config['smart_ds_years'][0]\n",
    "smart_ds_load_path = config['smart_ds_load_path'] + f\"/{smart_ds_year}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0a38b-a785-4cd5-b4a6-405649f179e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dictionary of measured building cooling and heating per feeder (from resstock parquet files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261c4a0e-60ce-43e4-b78f-de8f07559f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Loop through all city, region, year, and building type combinations\n",
    "for city, regions in CITY_REGIONS_TO_RUN.items():\n",
    "    if aggregation_level != 'feeder':\n",
    "        raise ValueError(\"This code supports feeder-level aggregation only - check aggregation_level value in config file\")\n",
    "    for region in regions:\n",
    "        # Initialize dictionary to store region results\n",
    "        measured_buildings_cool_heat_dict = {}\n",
    "        ### Import and process smart-ds load data ###\n",
    "        parquet_data_path = f'main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/load_data' # path to parquet files\n",
    "        region_directory = f\"main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/scenarios/base_timeseries/opendss/\" # path to where feeder folders are\n",
    "        # Create a list of load_models (regional / feeders / buildings)   \n",
    "        load_models = input_ops.make_feeder_list(region_directory) # list of feeders\n",
    "        print(f'......Creating data frame for {smart_ds_year} {city} {region} ......')\n",
    "        for load_model in load_models: # for feeder in feeders list\n",
    "            for building_type in building_types:\n",
    "                print(f'......Creating data frames for {smart_ds_year} {city} {region} feeder {load_model} {building_type}......')\n",
    "                ### Get all parquet files from load_data folder (all res/com stock profiles in the region) ###\n",
    "                parquet_files = sorted([f for f in os.listdir(parquet_data_path) if f.startswith(building_type) and f.endswith(\".parquet\")])\n",
    "                feeder_path_name = input_ops.add_feeder_upper_folder(load_model)\n",
    "                feeder_path = f'main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/scenarios/base_timeseries/opendss/{feeder_path_name}'\n",
    "                ## filter parquet_files list and keep only res/com stock profiles that exist in the feeder (Extract valid names from Loads.dss)  \n",
    "                valid_names = set()\n",
    "                file_path = feeder_path + \"/Loads.dss\"\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        match = re.search(r\"yearly=(res|com)_kw_(\\d+)\", line)\n",
    "                        if match:\n",
    "                            valid_names.add(f\"{match.group(1)}_{match.group(2)}\")  # e.g., Extract \"res_376\" or \"com_10111\"\n",
    "                # Filter parquet_files based on feeder's loads.dss and skip to next feeder if non exist\n",
    "                parquet_files = [f for f in parquet_files if f.replace(\".parquet\", \"\") in valid_names]\n",
    "                if not parquet_files:\n",
    "                    print(f\"Feeder {feeder_path} skipped since no parquet files found in the given folder with the specified prefix.\")                            \n",
    "                    continue \n",
    "                    \n",
    "                # Initialize dictionary for this feeder and building type\n",
    "                measured_buildings_cool_heat_dict[(smart_ds_year, city, region, load_model, building_type)] = {}\n",
    "\n",
    "                # Loop through all parquet files in feeder, process them and add to dictionary\n",
    "                for file in parquet_files:\n",
    "                    parquet_file_path = os.path.join(parquet_data_path, file)\n",
    "                    load_prefix = file.split('_')[0]\n",
    "\n",
    "                    if load_prefix not in {'com', 'res'}:\n",
    "                        print(f\"Parquet file {file} skipped since it didn't start with 'com' or 'res'\")\n",
    "                        continue\n",
    "\n",
    "                    load_df = input_ops.get_parquet_load_data(parquet_data_path, parquet_file_path, start_month, end_month)\n",
    "                    load_df = input_ops.convert_columns_to_CH_and_non_CH(load_df)\n",
    "                    load_df = load_df.set_index('date_time')\n",
    "\n",
    "                    building_id = file.split('.')[0]\n",
    "                    measured_buildings_cool_heat_dict[(smart_ds_year, city, region, load_model, building_type)][building_id] = load_df\n",
    "\n",
    "        # Save joblib for city-region\n",
    "        print(f'saving joblib for city {city} region {region}')\n",
    "        input_data_region_dir = f'{smart_ds_load_path}/{city}/{region}/buildings'\n",
    "        os.makedirs(input_data_region_dir, exist_ok=True)     # Ensure the directory exists\n",
    "        joblib.dump(measured_buildings_cool_heat_dict, os.path.join(input_data_region_dir, \"measured_buildings_cool_heat_dict.joblib\")) # Save the file\n",
    "end_time = time.time(); print(f\"Runtime for loading data: {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b94e56-4a15-4636-bd73-49cdf0e9871f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load measured buildings cooling heating dict (single region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5dd6b4-b026-427b-a498-8f8ac10bba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'GSO'\n",
    "region = 'rural'\n",
    "smart_ds_year = config['smart_ds_years'][0]\n",
    "smart_ds_load_path = config['smart_ds_load_path'] + f\"/{smart_ds_year}\"\n",
    "input_data_region_dir = f'{smart_ds_load_path}/{city}/{region}/buildings'\n",
    "measured_buildings_cool_heat_dict = joblib.load(os.path.join(input_data_region_dir, \"measured_buildings_cool_heat_dict.joblib\")) # Save the file\n",
    "\n",
    "measured_buildings_cool_heat_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendss_env2",
   "language": "python",
   "name": "opendss_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
