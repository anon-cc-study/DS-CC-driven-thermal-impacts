{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ee70d7-e471-4bba-a190-52312d6821a5",
   "metadata": {},
   "source": [
    "## Process smart-ds load and resstock weather data and prepare input data for regression model training\n",
    "\n",
    "This notebook does the following:\n",
    "1. Import weather and load data\n",
    "2. Aggregate load data\n",
    "3. Save weather and load data in a dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fac54-1aa2-4ff9-9d93-e197cc887ea6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd4c2f3-c8c4-4b62-a794-34301e021636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "from src import figure_ops\n",
    "from src import input_ops\n",
    "from src import model_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc8f26-a41f-4047-972d-aaf236f989fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load config file with scenarios and parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc43eb4-3909-43df-8cf4-f024ef6b686b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file_name = 'config1'; config_path = f\"config/{config_file_name}.yaml\"; config = input_ops.load_config(config_path)\n",
    "\n",
    "input_data_training_path = config['input_data_training_path']\n",
    "smart_ds_years = config['smart_ds_years']\n",
    "CITY_REGIONS_TO_RUN = config['CITY_REGIONS_TO_RUN']\n",
    "aggregation_level = config['aggregation_level']\n",
    "building_types = config['building_types']\n",
    "start_month = config['start_month']\n",
    "end_month = config['end_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c212e-cc78-4153-9006-e926165963f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import, process and save input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105c1914-9977-4e8c-8218-e2727a64dd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import, process and save load and weather data (run only once per dataset, e.g., run again to add new regions or years)\n",
    "start_time = time.time()\n",
    "\n",
    "# # Define dictionary to store results\n",
    "input_data_dict = {}\n",
    "\n",
    "# Loop through all city, region, year, and building type combinations\n",
    "for smart_ds_year in smart_ds_years:\n",
    "    for city, regions in CITY_REGIONS_TO_RUN.items():\n",
    "        for region in regions:\n",
    "            ### Import and process resstock weather data ###\n",
    "            # Load weather file based on city-reigon that matches ResStock county (FIPS) \n",
    "            match city:\n",
    "                case 'SFO':\n",
    "                    weather_file = f'G0600750_{smart_ds_year}.csv'  # San-Francisco county\n",
    "                case 'AUS':\n",
    "                    weather_file = f'G4800150_{smart_ds_year}.csv'  # Austin county\n",
    "                case _:\n",
    "                    weather_file = f'G3700810_{smart_ds_year}.csv'  # Greensboro Guilford county\n",
    "            weather_data_path = f'main_folder/OpenDSS/raw_data/resstock_weather/amy{smart_ds_year}/{city}/{weather_file}'\n",
    "            weather_df = input_ops.import_resstock_weather_data(weather_data_path, start_month, end_month)\n",
    "            \n",
    "            ### Import and process smart-ds load data ###\n",
    "            parquet_data_path = f'main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/load_data' # path to parquet files\n",
    "            region_directory = f\"main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/scenarios/base_timeseries/opendss/\" # path to where feeder folders are\n",
    "            # Create a list of load_models (regional / feeders / buildings)   \n",
    "            match aggregation_level:\n",
    "                case 'regional':\n",
    "                    load_models = ['regional']\n",
    "                case 'feeder':\n",
    "                    # load_models = find_feeder_folders(region_directory) # list of feeders in the format of '/substation/feeder/'\n",
    "                    load_models = input_ops.make_feeder_list(region_directory) # list of feeders\n",
    "                case _: # building\n",
    "                    load_models = sorted([f for f in os.listdir(parquet_data_path) if f.endswith(\".parquet\")]) # list of buildings in the format of 'building_name.parquet' \n",
    "            print(f'......Creating data frame for {smart_ds_year} {city} {region} ......')\n",
    "            for load_model in load_models:\n",
    "                for building_type in building_types:\n",
    "                    print(f'......Creating data frame for {smart_ds_year} {city} {region} {load_model} {building_type}......')\n",
    "                    match aggregation_level:\n",
    "                        case 'regional':\n",
    "                            loads_dss_paths = input_ops.find_folders_with_file(region_directory, \"Loads.dss\")\n",
    "                            load_df = input_ops.aggregate_parquet_data(parquet_data_path, loads_dss_paths, building_type, aggregation_level, start_month, end_month)  # 'res' for residential | 'com' for commercial\n",
    "                        case 'feeder':\n",
    "                            feeder_path_name = input_ops.add_feeder_upper_folder(load_model)\n",
    "                            feeder_path = f'main_folder/SMART-DS/v1.0/{smart_ds_year}/{city}/{region}/scenarios/base_timeseries/opendss/{feeder_path_name}'\n",
    "                            loads_dss_paths = [feeder_path]\n",
    "                            load_df = input_ops.aggregate_parquet_data(parquet_data_path, loads_dss_paths, building_type, aggregation_level, start_month, end_month)  # 'res' for residential | 'com' for commercial\n",
    "                        case _: # building\n",
    "                            load_prefix = load_model.split('_')[0]\n",
    "                            if load_prefix not in {'com', 'res'}:\n",
    "                                continue\n",
    "                            load_df = input_ops.get_parquet_load_data(parquet_data_path, load_model,start_month,end_month)\n",
    "                    if load_df.empty:\n",
    "                        print(f\"load_df is empty, perhaps parquet files were not found in the given folder with the specified prefix.\")\n",
    "                        continue \n",
    "\n",
    "                    # Reset index in load df (since 'building_id' is set as an index and not date time as in weather df)\n",
    "                    load_df = load_df.reset_index()\n",
    "                    # Merge data\n",
    "                    input_df = input_ops.merge_load_weather(load_df, weather_df)\n",
    "                    input_df.rename(columns={'month_x': 'month'}, inplace=True)\n",
    "                    # Store in dictionary with (year, city, region, load_model, building_type) as the key\n",
    "                    input_data_dict[(smart_ds_year, city, region, load_model, building_type)] = input_df\n",
    "            # Save dictionary of current region as joblib files\n",
    "            if aggregation_level == 'building':\n",
    "                input_data_region_dir = f'{input_data_training_path}/{city}/{region}/'\n",
    "                if not os.path.exists(input_data_region_dir):\n",
    "                    os.makedirs(input_data_region_dir, exist_ok=True)\n",
    "                print(f'saving joblib for {city} {region}')\n",
    "                joblib.dump(input_data_dict, os.path.join(input_data_region_dir, f\"input_data_dict.joblib\"))                  \n",
    "                     \n",
    "# Save dictionary of all regions as joblib files\n",
    "if aggregation_level == 'regional' or aggregation_level == 'feeder':\n",
    "    print('saving joblib for all regions')\n",
    "    os.makedirs(input_data_training_path, exist_ok=True)     # Ensure the directory exists\n",
    "    joblib.dump(input_data_dict, os.path.join(input_data_training_path, \"input_data_dict.joblib\")) # Save the file\n",
    "\n",
    "end_time = time.time(); print(f\"Runtime for loading data: {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendss_env2",
   "language": "python",
   "name": "opendss_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
