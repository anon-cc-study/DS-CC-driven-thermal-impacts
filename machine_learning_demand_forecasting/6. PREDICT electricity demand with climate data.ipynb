{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fd26e4-423d-4836-81d2-e0dd8752810a",
   "metadata": {},
   "source": [
    "## Predict feeder electricity demand time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d132a-cdc1-4fc2-a411-b902fdf34682",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eaaaebf-4ca3-4579-bed5-f3e464ff25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from pandas import DatetimeIndex\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import sklearn\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "from src import input_ops\n",
    "from src import model_ops\n",
    "from src import aux_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb2f57-b524-4f05-8116-3b4a014d7c92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load config file with scenarios and parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc84cf5f-9390-4ac4-8654-ec52b93250d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name = 'config1'; config_path = f\"config/{config_file_name}.yaml\"; config = input_ops.load_config(config_path)\n",
    "aggregation_level = config['aggregation_level']\n",
    "X_columns = config[config['X_columns_set']]\n",
    "input_data_prediction_path = config['input_data_prediction_path']\n",
    "print(f\"prediction model aggregation_level:{aggregation_level} \\nsmart_ds_years:{config['smart_ds_years'][0]} \\nmonths:{config['start_month']}_{config['end_month']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c3884-cae4-4249-ae83-c0d82aa9b0a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict future load using TGW weather data (cooling/heating) \n",
    "Note: should take about 10 min for all regions and scenarios (both cooling and heating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "CITY_REGIONS_TO_RUN = {\n",
    "    \"GSO\": [\"rural\"],\n",
    "}\n",
    "\n",
    "TGW_years_scenarios = { \n",
    "    \"2018\": [\"historical\"], \n",
    "}\n",
    "\n",
    "Y_columns = [\"cooling_kw_sum\",\"heating_kw\"]\n",
    "\n",
    "for Y_column in Y_columns:\n",
    "    print(f\"Starting prediction for {Y_column}\")\n",
    "    if Y_column == 'cooling_kw_sum':\n",
    "        prediction_model_str = config['prediction_model_cooling']\n",
    "        print(f\"with prediction model str {prediction_model_str}\")\n",
    "    elif Y_column == 'heating_kw':\n",
    "        prediction_model_str = config['prediction_model_heating']\n",
    "        print(f\"with prediction model str {prediction_model_str}\")\n",
    "    else:\n",
    "        raise ValueError('Model architecture is not defined for model output! (only for cooling_sum and heating)')\n",
    "    output_path_prediction_str = f\"main_folder/load_prediction/results/data/prediction/output/{config['smart_ds_years'][0]}/months_{config['start_month']}_{config['end_month']}/{Y_column}/{config['X_columns_set']}/{config['aggregation_level']}/\" \n",
    "    output_data_training_path_str = f\"main_folder/load_prediction/results/data/training/{config['smart_ds_years'][0]}/months_{config['start_month']}_{config['end_month']}/ml_output_data/{Y_column}/{config['X_columns_set']}/{config['aggregation_level']}/\"\n",
    "    print(output_data_training_path_str)   \n",
    "    \n",
    "    # Load dictionaries containing models and scalers for all smart-ds regions\n",
    "    mlp_models = joblib.load(os.path.join(output_data_training_path_str, \"models\", f\"{prediction_model_str}_models_dict.joblib\"))\n",
    "    xnorms = joblib.load(os.path.join(output_data_training_path_str, \"scalers\", f\"{prediction_model_str}_xnorm_dict.joblib\"))\n",
    "    ynorms = joblib.load(os.path.join(output_data_training_path_str, \"scalers\",f\"{prediction_model_str}_ynorm_dict.joblib\"))\n",
    "\n",
    "    smart_ds_years, cities, regions, load_models, building_types = aux_ops.extract_unique_dimensions(mlp_models,CITY_REGIONS_TO_RUN)\n",
    "    for TGW_weather_year, TGW_scenarios in TGW_years_scenarios.items():\n",
    "        for TGW_scenario in TGW_scenarios:\n",
    "            predictions_dict = {}     ## Initialize dictionary\n",
    "            print(f\"Running for year {TGW_weather_year} and scenario {TGW_scenario}\")\n",
    "            for smart_ds_year in smart_ds_years:\n",
    "                for city in cities:\n",
    "                    ## Load TGW weather data\n",
    "                    # Select TGW weather file based on smart-ds city\n",
    "                    match city:\n",
    "                        case 'GSO':\n",
    "                            TGW_location = 'Greensboro'\n",
    "                        case 'AUS':\n",
    "                              TGW_location = 'Austin'\n",
    "                        case 'SFO': \n",
    "                              TGW_location = 'SanFrancisco' \n",
    "                    # Load TGW weather input data \n",
    "                    TGW_weather_df_save_path = f\"{input_data_prediction_path}/{TGW_location}/{TGW_scenario}/\"\n",
    "                    TGW_weather_df = joblib.load(os.path.join(TGW_weather_df_save_path, f\"TGW_weather_{TGW_weather_year}.joblib\"))\n",
    "                    input_df_new = TGW_weather_df \n",
    "                    for region in regions:\n",
    "                        print(f\".....Predicting {Y_column} with model {prediction_model_str} trained with SMART-DS data from {smart_ds_year} {city} {region} .....\")\n",
    "                        for load_model in load_models:\n",
    "                            for building_type in building_types:\n",
    "                                key = (smart_ds_year, city, region, load_model, building_type)\n",
    "                                if key in mlp_models:\n",
    "                                    # Load feeder-specific model and scalers \n",
    "                                    ML_model = mlp_models[(smart_ds_year, city, region, load_model, building_type)]\n",
    "                                    xnorm = xnorms[(smart_ds_year, city, region, load_model, building_type)]\n",
    "                                    ynorm = ynorms[(smart_ds_year, city, region, load_model, building_type)]\n",
    "\n",
    "                                    ### Filter, normalize, predict and denormalize output ###\n",
    "                                    # Filter input data to inputs the model was trained on\n",
    "                                    X_new = input_df_new[X_columns]\n",
    "                                    # verify no NaNs or missing features\n",
    "                                    if X_new.isnull().any().any():\n",
    "                                        raise ValueError(f\"Missing values in input data for {key}\")\n",
    "                                    # Normalize new input data using the scaler that was used for training\n",
    "                                    X_new_norm = xnorm.transform(X_new)\n",
    "                                    # Predict normalized y values using the trained MLP model\n",
    "                                    MLP_y_new_pred_norm = ML_model.predict(X_new_norm).reshape(-1, 1)\n",
    "                                    # Denormalize the predicted y values back to the original scale\n",
    "                                    MLP_y_new_pred = ynorm.inverse_transform(MLP_y_new_pred_norm)\n",
    "                                    y_pred = MLP_y_new_pred\n",
    "\n",
    "                                    ### Save prediction with weather inputs and datetime ###\n",
    "                                    df_output = X_new.copy() # initialize df with input features\n",
    "                                    # Insert the new column at the beginning (index 0)\n",
    "                                    df_output.insert(0, \"date_time\", input_df_new[\"date_time\"])\n",
    "                                    df_output[f\"{Y_column}_predicted\"] = y_pred\n",
    "                                    predictions_dict[(TGW_weather_year, city, region, load_model, building_type)]  = df_output\n",
    "\n",
    "            # Save dictionaries as joblib files\n",
    "            predictions_dir = os.path.join(output_path_prediction_str, f\"{TGW_scenario}/predictions\")\n",
    "            os.makedirs(predictions_dir, exist_ok=True) # Create directories if they don't exist\n",
    "            joblib.dump(predictions_dict, os.path.join(predictions_dir, f\"{prediction_model_str}_TGW_{TGW_weather_year}_models_dict.joblib\"))\n",
    "            print(f\"Saved model with {TGW_weather_year} and {TGW_scenario} at path: \\n {predictions_dir} \\n with name: {prediction_model_str}_TGW_{TGW_weather_year}_models_dict.joblib\")\n",
    "\n",
    "print(\"All predictions were saved successfully!\")\n",
    "\n",
    "end_time = time.time(); print(f\"Runtime for Prediction: {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93356d-bbf3-4221-a2f6-d901769e2346",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGW_weather_year = '2018'\n",
    "TGW_scenario = 'historical'\n",
    "\n",
    "Y_column = \"cooling_kw_sum\" # cooling_kw_sum, heating_kw\n",
    "\n",
    "if Y_column == 'cooling_kw_sum':\n",
    "    prediction_model_str = config['prediction_model_cooling']\n",
    "elif Y_column == 'heating_kw':\n",
    "    prediction_model_str = config['prediction_model_heating']\n",
    "else:\n",
    "    raise ValueError('Model architecture is not defined for model output! (only for cooling_sum and heating)')\n",
    "output_path_prediction_str = f\"main_folder/load_prediction/results/data/prediction/output/{config['smart_ds_years'][0]}/months_{config['start_month']}_{config['end_month']}/{Y_column}/{config['X_columns_set']}/{config['aggregation_level']}/\" \n",
    "\n",
    "\n",
    "predictions_dir = os.path.join(output_path_prediction_str, f\"{TGW_scenario}/predictions\")\n",
    "# Load dictionary with predictions \n",
    "loaded_predictions_dict = joblib.load(os.path.join(predictions_dir, f\"{prediction_model_str}_TGW_{TGW_weather_year}_models_dict.joblib\"))\n",
    "print(f'Loading model from path: {predictions_dir} \\n named: {prediction_model_str}_TGW_{TGW_weather_year}_models_dict.joblib \\n with keys:')\n",
    "loaded_predictions_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendss_env2",
   "language": "python",
   "name": "opendss_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
